{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import json\n",
    "import scipy.stats as stats\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "from aml_dl.mdn.model.mdn_push_fwd_model import MDNPushFwdModel\n",
    "from config.shape_db import *\n",
    "import tf.transformations as tfm\n",
    "shape_db = ShapeDB()\n",
    "\n",
    "#from aml_data_collec_utils.core.data_manager import DataManager\n",
    "#from aml_dl.mdn.utilities.get_data_from_files import get_data_from_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "    \n",
    "adam_params = {\n",
    "    'type': 'adam',\n",
    "    'params': {'learning_rate' : 0.0001, 'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-08, 'use_locking': False}\n",
    "}\n",
    "\n",
    "network_params = {\n",
    "    'dim_input': 61, \n",
    "    'dim_output': 3,\n",
    "    'n_hidden': 400,\n",
    "    'k_mixtures': 200,\n",
    "    'write_summary': False,\n",
    "    'learning_rate': 0.00005,\n",
    "    'load_saved_model': False,\n",
    "    'optimiser': adam_params,\n",
    "    'device': '/cpu:0',\n",
    "    'dropout_prob': 0.8,\n",
    "    'weight_multiplier': 1.0, # changed iteratively later on only if halfweightValidation used (for dropout)\n",
    "    'weight_reg_coef': 0.001, # change to factor weight size regularization\n",
    "    'max_weight_mag': 1000, # set high to turn off weight contraining\n",
    "}\n",
    "\n",
    "forward_model = MDNPushFwdModel(sess=sess, network_params=network_params)\n",
    "forward_model.init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "json_filepath = '/home/harry/Honeycomb/surface_compare/delrin/rect1_json/data_training_with_shapeBounds_andForce.json'\n",
    "\n",
    "with open(json_filepath) as data_file:    \n",
    "\t\tdata_rect1 = json.load(data_file)\n",
    "        \n",
    "json_filepath = '/home/harry/Honeycomb/surface_compare/delrin/rect2_json/data_training_with_shapeBounds_andForce.json'\n",
    "with open(json_filepath) as data_file:    \n",
    "\t\tdata_rect2 = json.load(data_file)\n",
    "        \n",
    "json_filepath = '/home/harry/Honeycomb/surface_compare/delrin/rect3_json/data_training_with_shapeBounds_andForce.json'\n",
    "with open(json_filepath) as data_file:    \n",
    "\t\tdata_rect3 = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7917 =len(train_data) with all velos\n",
      "3960 =len(val_data) with all velos\n",
      "792 =len(train_data) with no velos\n",
      "396 =len(val_data) with no velos\n"
     ]
    }
   ],
   "source": [
    "# steps needed for the shapes dataset, remove once optimized for any data\n",
    "shape_ids = ['rect1']\n",
    "#----------------------CHANGE FOR DIFFERENT VAL SET---------------------------------        \n",
    "train_data = []\n",
    "train_data.extend(data_rect2)\n",
    "train_data.extend(data_rect3)\n",
    "\n",
    "val_data = []\n",
    "val_data.extend(data_rect1)\n",
    "\n",
    "print len(train_data), \"=len(train_data) with all velos\"\n",
    "print len(val_data), \"=len(val_data) with all velos\"\n",
    "\n",
    "# removes all velocity cases\n",
    "dataNew = []\n",
    "for i in range(len(train_data)):\n",
    "    if (train_data[i][13] == 10):\n",
    "        dataNew.append(train_data[i])\n",
    "#train_data = np.array(dataNew)\n",
    "train_data = dataNew\n",
    "print len(train_data), \"=len(train_data) with no velos\"\n",
    "\n",
    "dataNew = []\n",
    "for i in range(len(val_data)):\n",
    "    if (val_data[i][13] == 10):\n",
    "        dataNew.append(val_data[i])\n",
    "#val_data = np.array(dataNew)\n",
    "val_data = dataNew\n",
    "print len(val_data), \"=len(val_data) with no velos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(792, 61)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all_data = np.vstack((train_data, val_data))\n",
    "\n",
    "train_data = train_data[:, np.r_[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]] # inputs\n",
    "val_data =     val_data[:, np.r_[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]] # inputs\n",
    "\n",
    "all_data = np.vstack((train_data, val_data))\n",
    "mins = np.amin(all_data, axis = 0)\n",
    "maxs = np.amax(all_data, axis = 0)\n",
    "train_data = (train_data-mins)/(maxs-mins)\n",
    "val_data = (val_data-mins)/(maxs-mins)\n",
    "\n",
    "dataX =   train_data[:, np.r_[0, 1, 2, 3, 4, 5, 6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]] # inputs\n",
    "dataY =   train_data[:, [7,8, 9]] # end parameters\n",
    "\n",
    "dataX_val = val_data[:, np.r_[0, 1, 2, 3, 4, 5, 6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]] # inputs\n",
    "dataY_val = val_data[:, [7,8, 9]] # end parameters\n",
    "\n",
    "np.shape(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harry/.virtualenvs/robotics/lib/python2.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  \n",
      "/home/harry/.virtualenvs/robotics/lib/python2.7/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in divide\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1188, 61)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rescale (min max normalise)\n",
    "all_data = train_data\n",
    "all_data.extend(val_data)\n",
    "all_data = np.array(all_data)\n",
    "mins = np.amin(all_data, axis = 0)\n",
    "maxs = np.amax(all_data, axis = 0)\n",
    "\n",
    "train_data = (train_data-mins)/(maxs-mins)\n",
    "val_data = (val_data-mins)/(maxs-mins)\n",
    "\n",
    "Ymins = np.r_[mins[7:10]] # 7,8,9\n",
    "Ymaxs = np.r_[maxs[7:10]]\n",
    "Xmins = np.r_[mins[0:7], mins[10:13], mins[14:65]] # 1,2,3,4,5,6,10,11,12,14\n",
    "Xmaxs = np.r_[maxs[0:7], maxs[10:13], maxs[14:65]]\n",
    "    \n",
    "dataX = train_data[:, np.r_[0, 1, 2, 3, 4, 5, 6, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]] # inputs\n",
    "dataY = train_data[:, [7,8, 9]] # end parameters\n",
    "    \n",
    "dataX_val = val_data[:, np.r_[0, 1, 2, 3, 4, 5, 6, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]] # inputs\n",
    "dataY_val = val_data[:, [7,8, 9]] # end parameters\n",
    "\n",
    "np.shape(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#all_data = np.vstack((train_data, val_data))\n",
    "# rescale (standardised variance, mean 0)\n",
    "\n",
    "#means = np.mean(all_data, axis = 0).reshape(1,len(all_data[1])) # vector length shape(1,d)\n",
    "#mean_translations = all_data - np.dot(np.ones((len(all_data),1)), means) # array size shape(M,d)\n",
    "#variances = np.mean(mean_translations**2, axis = 0).reshape(1,len(all_data[1])) # vector length shape(1,d)\n",
    "\n",
    "#train_data = np.divide((train_data - np.dot(np.ones((len(train_data),1)), means)),variances)\n",
    "#dataX = train_data[:, np.r_[0, 1, 2, 3, 4, 5, 6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]] # inputs\n",
    "#dataY = train_data[:, [7,8, 9]] # end parameters\n",
    "\n",
    "#val_data = np.divide((val_data - np.dot(np.ones((len(val_data),1)), means)), variances)\n",
    "#dataX_val = val_data[:, np.r_[0, 1, 2, 3, 4, 5, 6, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]] # inputs\n",
    "#dataY_val = val_data[:, [7,8, 9]] # end parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rescale (min max normalise)\n",
    "all_data = train_data\n",
    "all_data.extend(val_data)\n",
    "all_data = np.array(all_data)\n",
    "mins = np.amin(all_data, axis = 0)\n",
    "maxs = np.amax(all_data, axis = 0)\n",
    "\n",
    "train_data = (train_data-mins)/(maxs-mins)\n",
    "val_data = (val_data-mins)/(maxs-mins)\n",
    "\n",
    "Ymins = np.r_[mins[7:10]] # 7,8,9\n",
    "Ymaxs = np.r_[maxs[7:10]]\n",
    "Xmins = np.r_[mins[0:7], mins[10:13], mins[14:65]] # 1,2,3,4,5,6,10,11,12,14\n",
    "Xmaxs = np.r_[maxs[0:7], maxs[10:13], maxs[14:65]]\n",
    "    \n",
    "dataX = train_data[:, np.r_[0, 1, 2, 3, 4, 5, 6, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]] # inputs\n",
    "dataY = train_data[:, [7,8, 9]] # end parameters\n",
    "    \n",
    "dataX_val = val_data[:, np.r_[0, 1, 2, 3, 4, 5, 6, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]] # inputs\n",
    "dataY_val = val_data[:, [7,8, 9]] # end parameters\n",
    "\n",
    "print np.shape(train_data), '= train_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_data[:,27], train_data[:,34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "\n",
    "training_params = {\n",
    "    'epochs': 2000, \n",
    "    'val_step': 100, # at what intervals to do validation, divisible by NEPOCH\n",
    "    'stoch_samples': 50, # how many stochastic dropout samples to use for a mean output reading \n",
    "    'minibatch': 25, # size of minibatches\n",
    "    'train_x': dataX,\n",
    "    'train_y': dataY,\n",
    "    'val_x': dataX_val,\n",
    "    'val_y': dataY_val,\n",
    "}\n",
    "\n",
    "forward_model.train_net(training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "loss = np.zeros(NEPOCH)\n",
    "val_loss = np.zeros(NEPOCH/val_step)\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    train_step = forward_model._net_ops['train']\n",
    "    loss_op  = forward_model._net_ops['loss']\n",
    "    bs = forward_model._params['batch_size']\n",
    "    val_counter = 0\n",
    "    counter = 0\n",
    "    \n",
    "    for i in range(NEPOCH): \n",
    "        if (i%25==0):\n",
    "            print i\n",
    "        b_start_counter = 0\n",
    "        b_end_counter = bs\n",
    "        \n",
    "        feed_dict = {forward_model._net_ops['x']: dataX[b_start_counter:b_end_counter], forward_model._net_ops['y']: dataY[b_start_counter:b_end_counter]}\n",
    "        feed_dict_val = {forward_model._net_ops['x']: dataX_val[0:bs], forward_model._net_ops['y']: dataY_val[0:bs]}\n",
    "        \n",
    "        for j in range(int(len(dataX))/bs):\n",
    "            _, loss[i] = forward_model._sess.run([train_step, loss_op], feed_dict=feed_dict)\n",
    "            \n",
    "            b_start_counter += bs\n",
    "            b_end_counter += bs\n",
    "            #shift batch on\n",
    "            feed_dict = {forward_model._net_ops['x']: dataX[b_start_counter:b_end_counter], forward_model._net_ops['y']: dataY[b_start_counter:b_end_counter]}\n",
    "        \n",
    "        if (i%val_step == 0):\n",
    "            valSumSum = 0\n",
    "            for j in range(int(len(dataX_val))):\n",
    "                valSum = 0\n",
    "                feed_dict_val = {forward_model._net_ops['x']: [dataX_val[j]], forward_model._net_ops['y']: [dataY_val[j]]}\n",
    "                for l in range(stoch_samples):\n",
    "                    valSum += forward_model._sess.run(loss_op, feed_dict=feed_dict_val)\n",
    "                valSumSum += valSum/stoch_samples\n",
    "            val_loss[val_counter] = valSumSum/len(dataX_val)\n",
    "            val_counter += 1\n",
    "            \n",
    "        #reshuffle after epoch to avoid same cycle\n",
    "        np.random.shuffle(train_data)\n",
    "        dataX = train_data[:, np.r_[0, 1, 2, 3, 4, 5, 6, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]] # inputs\n",
    "        dataY = train_data[:, [7,8, 9]] # end parameters\n",
    "            \n",
    "        np.random.shuffle(val_data)\n",
    "        dataX_val = val_data[:, np.r_[0, 1, 2, 3, 4, 5, 6, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]] # inputs\n",
    "        dataY_val = val_data[:, [7,8, 9]] # end parameters\n",
    "        \n",
    "        if (i%val_step == 0) and (i>1):\n",
    "            #print loss[i]\n",
    "            #print val_loss[i]\n",
    "            plt.plot(range(0,i), loss[0:i], label = \"loss\")\n",
    "            plt.plot(range(0,i+1,val_step), val_loss[0:val_counter], label = \"validation loss\")\n",
    "            plt.show()\n",
    "            \n",
    "        counter +=1\n",
    "        #print 'epoch: ',i+1, ', loss: ', round(loss[i], 3), ', last calculated val loss: ',round(val_loss[val_counter-1], 3), 'time elapsed: ',round((time.time()-t0)/60), 'minutes' \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in range(4): \n",
    "    shape_id = shape_ids[n]\n",
    "    probe_radius = 0.004745\n",
    "    \n",
    "    if (shape_id == 'ellip1' or shape_id == 'ellip2' or shape_id == 'ellip3' or shape_id == 'butter'):\n",
    "        shape_polygon = shape_db.shape_db[shape_id]['shape'][0]\n",
    "    else:\n",
    "        shape_polygon = shape_db.shape_db[shape_id]['shape']\n",
    "        shape_polygon.append(shape_polygon[0])\n",
    "     \n",
    "    # average over multiple stochastic models to get averaged\n",
    "    feed_dict = {forward_model._net_ops['x']: dataX_vals[n]}\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        ccpredictions_mus = []\n",
    "    \n",
    "        for i in range(stoch_samples*10):\n",
    "            mus_op = forward_model._net_ops['mu']\n",
    "            sigmas_op  = forward_model._net_ops['sigma']\n",
    "            pis_op = forward_model._net_ops['pi']\n",
    "    \n",
    "            out_mus = forward_model._sess.run(mus_op, feed_dict=feed_dict)\n",
    "            out_sigmas = forward_model._sess.run(sigmas_op, feed_dict=feed_dict)\n",
    "            out_pis = forward_model._sess.run(pis_op, feed_dict=feed_dict)\n",
    "    \n",
    "            concat_out_pis = np.swapaxes(np.array([out_pis, out_pis, out_pis]),0,1) # make shape same as mus\n",
    "            weighted_mus = out_mus*concat_out_pis\n",
    "            ccpredictions_mus.append(np.sum(weighted_mus,2).tolist())\n",
    "    \n",
    "        sum_avg_err_NN = np.sum(np.mean(np.abs(dataY_vals[n]-np.mean(ccpredictions_mus, axis = 0)), axis = 0))\n",
    "    \n",
    "        # de_rescale \n",
    "        ccpredictions_mus = (ccpredictions_mus*(Ymaxs-Ymins))+Ymins\n",
    "        dataX_val = (dataX_vals[n]*(Xmaxs-Xmins))+Xmins  \n",
    "        dataY_val = (dataY_vals[n]*(Ymaxs-Ymins))+Ymins\n",
    "        \n",
    "        ccpredictions_mus_mean = np.mean(ccpredictions_mus, axis = 0)\n",
    "        ccpredictions_sigmas = np.std(ccpredictions_mus, axis = 0)\n",
    "        \n",
    "        #better error measure\n",
    "        dXYtheta = dataY_val-ccpredictions_mus_mean\n",
    "        hypoten = []\n",
    "        for e in range(len(dXYtheta)):\n",
    "            dist = np.sqrt(np.square(dXYtheta[e][0])+np.square(dXYtheta[e][1]))\n",
    "            hypoten.append(dist)\n",
    "        DIST_avg_err_NN = np.mean(hypoten)\n",
    "        DIST_std_err_NN = np.std(hypoten)\n",
    "        ROTA_avg_err_NN = np.mean(np.abs(dXYtheta[:,2]))\n",
    "        ROTA_std_err_NN = np.std(np.abs(dXYtheta[:,2]))\n",
    "        \n",
    "    print sum_avg_err_NN, '= sum_avg_err_NN for shape', shape_id\n",
    "    print DIST_avg_err_NN, '= DIST_avg_err_NN for shape', shape_id\n",
    "    print DIST_std_err_NN, '= DIST_std_err_NN for shape', shape_id\n",
    "    print ROTA_avg_err_NN, '= ROTA_avg_err_NN for shape', shape_id\n",
    "    print ROTA_std_err_NN, '= ROTA_std_err_NN for shape', shape_id\n",
    "    \n",
    "    dataX_test = dataX_val\n",
    "    dataY_test = dataY_val\n",
    "    counter = 0\n",
    "    \n",
    "    if (shape_id!='ellip1' and shape_id!='ellip2' and shape_id!='ellip3'):\n",
    "        for i in range(len(dataX_test)):\n",
    "            obj_start = dataX_test[i, [4, 5, 6]]\n",
    "            transformed_start = rigidtransform(shape_polygon, obj_start)\n",
    "            plottransformed_obj(transformed_start, 'g', 1.0)\n",
    "            plottransformed_pusher(dataX_test[i,0], dataX_test[i,1], probe_radius, 'g')\n",
    "\n",
    "            transformed_end = rigidtransform(shape_polygon, dataY_test[i])\n",
    "            plottransformed_obj(transformed_end, 'r', 1.0)\n",
    "            plottransformed_pusher(dataX_test[i,2], dataX_test[i,3], probe_radius, 'r')\n",
    "\n",
    "            pred = ccpredictions_mus_mean[i] ## uncomment to use convex combination\n",
    "            pred_sig = ccpredictions_sigmas[i]\n",
    "            #pred = predictions[i]\n",
    "    \n",
    "            xs = np.linspace(pred[0]-pred_sig[0], pred[0]+pred_sig[0], 3)\n",
    "            ys = np.linspace(pred[1]-pred_sig[1], pred[1]+pred_sig[1], 3)\n",
    "            thetas = np.linspace(pred[2]-pred_sig[2], pred[2]+pred_sig[2], 3)\n",
    "    \n",
    "            for i in xs:\n",
    "                for j in ys:\n",
    "                    for l in thetas:\n",
    "                        transformed_pred = rigidtransform(shape_polygon, [i,j,l])\n",
    "                        plottransformed_obj(transformed_pred, 'b', 0.2)\n",
    "                \n",
    "            transformed_pred = rigidtransform(shape_polygon, pred)\n",
    "            plottransformed_obj(transformed_pred, 'b', 1)\n",
    "                \n",
    "            print 'NN'\n",
    "            plt.show()            \n",
    "    else:\n",
    "        for i in range(len(dataX_test)):\n",
    "            obj_startX = dataX_test[i, [4]]\n",
    "            obj_startY = dataX_test[i, [5]]\n",
    "            obj_startTheta = dataX_test[i, [6]]\n",
    "            obj_endX = dataY_test[i, [0]]\n",
    "            obj_endY = dataY_test[i, [1]]\n",
    "            obj_endTheta = dataY_test[i, [2]]\n",
    "    \n",
    "            pred = ccpredictions_mus_mean[i] ## uncomment to use convex combination\n",
    "            pred_sig = ccpredictions_sigmas[i]\n",
    "    \n",
    "            plt.figure(figsize=(4, 4))\n",
    "    \n",
    "            plt.figure(i).add_subplot(111, aspect='equal').add_artist(Ellipse((obj_startX, obj_startY), shape_polygon[0]*2, shape_polygon[1]*2, angle=math.degrees(obj_startTheta), fill=False, color='g'))\n",
    "            plottransformed_pusher(dataX_test[i,0], dataX_test[i,1], probe_radius, 'g')\n",
    "            plt.figure(i).add_subplot(111, aspect='equal').add_artist(Ellipse((obj_endX, obj_endY), shape_polygon[0]*2, shape_polygon[1]*2, angle=math.degrees(obj_endTheta), fill=False, color='r'))\n",
    "            plottransformed_pusher(dataX_test[i,2], dataX_test[i,3], probe_radius, 'r')\n",
    "    \n",
    "            xs = np.linspace(pred[0]-pred_sig[0], pred[0]+pred_sig[0], 3)\n",
    "            ys = np.linspace(pred[1]-pred_sig[1], pred[1]+pred_sig[1], 3)\n",
    "            thetas = np.linspace(pred[2]-pred_sig[2], pred[2]+pred_sig[2], 3)\n",
    "    \n",
    "            for m in xs:\n",
    "                for j in ys:\n",
    "                    for l in thetas:\n",
    "                        plt.figure(i).add_subplot(111, aspect='equal').add_artist(Ellipse((m, j), shape_polygon[0]*2, shape_polygon[1]*2, angle=math.degrees(l), fill=False, color='b', alpha = 0.1))\n",
    "                \n",
    "            print 'NN'\n",
    "            plt.figure(i).add_subplot(111, aspect='equal').add_artist(Ellipse((pred[0], pred[1]), shape_polygon[0]*2, shape_polygon[1]*2, angle=math.degrees(pred[2]), fill=False, color='b', alpha = 1))\n",
    "            plt.axis([obj_startX-0.15, obj_startX+0.15, obj_startY-0.15, obj_startY+0.15])\n",
    "            plt.show()        \n",
    "        print counter\n",
    "        counter +=1\n",
    "    t1 = time.time()\n",
    "\n",
    "    total = t1-t0\n",
    "    print total/60 #time in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_mean = np.mean(dataY_test, axis=0)\n",
    "end_obs = dataY_test\n",
    "end_pred = ccpredictions_mus_mean\n",
    "\n",
    "pred_diff_squared = np.square(end_obs-end_pred)\n",
    "obs_diff_squared = np.square(end_obs-y_mean)\n",
    "\n",
    "n_summed_obs = np.sum(obs_diff_squared, axis=0)\n",
    "n_summed_pred = np.sum(pred_diff_squared, axis=0)\n",
    "\n",
    "nmse = n_summed_pred/n_summed_obs\n",
    "print nmse\n",
    "print np.sum(nmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_mean = np.mean(dataY_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "width = 0.35\n",
    "\n",
    "self_dists_samples =  [[0.0051222339288334263, 0.0044158771643205835, 0.0055815628117350888],[0.0055966727612206321, 0.0052129928893983784, 0.0053804816110431036],[0.0055966727612206321, 0.0052129928893983784, 0.0053804816110431036],[0.0062445242198103552, 0.0077418373545106618, 0.0061277925049182556]]\n",
    "self_rots_samples =  [[0.11908324792858131, 0.11306688419043792, 0.13449593724190473],[0.13315670483678643, 0.15913810546391505, 0.16429507204279439],[0.13315670483678643, 0.15913810546391505, 0.16429507204279439],[0.16339293662301596, 0.20019611339372209, 0.1321339022201474]]\n",
    "self_dists_std = (np.std(self_dists_samples, axis=1)*1000)/np.sqrt(40)\n",
    "self_rots_std = np.std(self_rots_samples, axis=1)/np.sqrt(40)\n",
    "self_dists = (np.mean(self_dists_samples, axis=1)*1000)\n",
    "self_rots = np.mean(self_rots_samples, axis=1)\n",
    "\n",
    "dists = np.array([0.00376513128265,0.00569862031285,0.00400732900878,0.00513413780176])*1000\n",
    "rots = np.array([0.114779675876,0.1966752851,0.12398435151,0.162222983751])\n",
    "dists_std = (np.array([0.00215905836733,0.00363459768816,0.00224951820607,0.00336522636219])*1000)/np.sqrt(400)\n",
    "rots_std = (np.array([0.181019167684,0.358298909743,0.231339061904,0.228280870148]))/np.sqrt(400)\n",
    "ind = np.arange(len(dists))\n",
    "# distances x 1000 to scale it from m to mm\n",
    "#fig, ax3 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "fig, dudax = plt.subplots()\n",
    "width = 0.2\n",
    "bar1 = ax1.bar(ind-width-width, dists, width, color='b', yerr=dists_std, capsize=3, hatch=\"*\")\n",
    "bar2 = ax1.bar(ind-width, self_dists, width, color='b', yerr=self_dists_std, capsize=3)\n",
    "\n",
    "ax1.set_xlabel('Shape')\n",
    "ax1.set_title('')\n",
    "ax1.set_xticks(ind + width/2 + width)\n",
    "ax1.set_xticklabels(('rect1', 'ellip1', 'tri1', 'butter'))\n",
    "ax1.set_ylabel('Avg Translation Error (mm)', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "\n",
    "dudbar1 = dudax.bar(ind, dists, width, color=\"1\", hatch=\"*\", edgecolor = \"0\")\n",
    "dudbar2 = dudax.bar(ind, dists, width, color=\"1\", edgecolor = \"0\")\n",
    "ax1.legend((dudbar1[0], dudbar2[0]), ('Shape-generalized', 'Self-generalized'))\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "bar3 = ax2.bar(ind, rots, width, yerr=rots_std, capsize=3, color = 'r', hatch=\"*\")\n",
    "bar4 = ax2.bar(ind+width, self_rots, width, yerr=self_rots_std, capsize=3, color = 'r')\n",
    "ax2.set_xticks(ind + width/2 - width)\n",
    "rotationname = 'Avg Rotation Error (' + r'$\\theta$)'\n",
    "ax2.set_ylabel(rotationname, color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "#ax2.set_ylim([0, 0.6])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax3.bar(ind-width, dists, width, yerr=dists_std, capsize=3, color='b')\n",
    "ax3.set_xlabel('Shape')\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax3.set_ylabel('Avg Translation Error (mm)', color='b')\n",
    "ax3.tick_params('y', colors='b')\n",
    "ax3.set_xticks(ind-width/2)\n",
    "ax3.set_xticklabels(('rect1', 'ellip1', 'tri1', 'butter'))\n",
    "\n",
    "ax4 = ax3.twinx()\n",
    "ax4.bar(ind, rots, width, yerr=rots_std, capsize=3, color = 'r')\n",
    "rotationname = 'Avg Rotation Error (' + r'$\\theta$)'\n",
    "ax4.set_ylabel(rotationname, color='r')\n",
    "ax4.tick_params('y', colors='r')\n",
    "ax4.set_ylim([0, 0.6])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape_pred_dist_err_NN =  [[0.0051222339288334263, 0.0044158771643205835, 0.0055815628117350888],[0.0055966727612206321, 0.0052129928893983784, 0.0053804816110431036],[0.0055966727612206321, 0.0052129928893983784, 0.0053804816110431036],[0.0062445242198103552, 0.0077418373545106618, 0.0061277925049182556]]\n",
    "shape_pred_rota_err_NN =  [[0.11908324792858131, 0.11306688419043792, 0.13449593724190473],[0.13315670483678643, 0.15913810546391505, 0.16429507204279439],[0.13315670483678643, 0.15913810546391505, 0.16429507204279439],[0.16339293662301596, 0.20019611339372209, 0.1321339022201474]]\n",
    "shape_pred_dist_err_KDE_silvermann =  [[0.0076849848465869028, 0.0061239361354140827, 0.0066136406935347405],[0.007095955688400513, 0.0067535231127581379, 0.0070346329249749533],[0.007095955688400513, 0.0067535231127581379, 0.0070346329249749533],[0.0067218774124073203, 0.0071787339605984693, 0.0068800714419015047]]\n",
    "shape_pred_rota_err_KDE_silvermann =  [[0.1628712066097272, 0.18399190153472958, 0.14635497744347506],[0.17927241083699877, 0.14591283781941025, 0.1874788206645388],[0.17927241083699877, 0.14591283781941025, 0.1874788206645388],[0.18773167922290554, 0.19606564396288928, 0.16156655221565774]]\n",
    "\n",
    "butter\n",
    "shape_pred_dist_err_NN =  []\n",
    "shape_pred_rota_err_NN =  []\n",
    "shape_pred_dist_err_KDE_silvermann =  [[\n",
    "shape_pred_rota_err_KDE_silvermann =  []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "width = 0.35\n",
    "dists = np.array([0.00376513128265,0.00569862031285,0.00400732900878,0.00513413780176])*1000\n",
    "rots = np.array([0.114779675876,0.1966752851,0.12398435151,0.162222983751])\n",
    "dists_std = (np.array([0.00215905836733,0.00363459768816,0.00224951820607,0.00336522636219])*1000)/np.sqrt(400)\n",
    "rots_std = (np.array([0.181019167684,0.358298909743,0.231339061904,0.228280870148]))/np.sqrt(400)\n",
    "ind = np.arange(len(dists))\n",
    "# distances x 1000 to scale it from m to mm\n",
    "#fig, ax3 = plt.subplots(figsize=(10, 5))\n",
    "fig, ax3 = plt.subplots()\n",
    "\n",
    "ax3.bar(ind-width, dists, width, yerr=dists_std, capsize=3, color='b')\n",
    "ax3.set_xlabel('Shape')\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax3.set_ylabel('Avg Translation Error (mm)', color='b')\n",
    "ax3.tick_params('y', colors='b')\n",
    "ax3.set_xticks(ind-width/2)\n",
    "ax3.set_xticklabels(('rect1', 'ellip1', 'tri1', 'butter'))\n",
    "\n",
    "ax4 = ax3.twinx()\n",
    "ax4.bar(ind, rots, width, yerr=rots_std, capsize=3, color = 'r')\n",
    "rotationname = 'Avg Rotation Error (' + r'$\\theta$)'\n",
    "ax4.set_ylabel(rotationname, color='r')\n",
    "ax4.tick_params('y', colors='r')\n",
    "ax4.set_ylim([0, 0.6])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dist and rotation errors from shape gen results of rect1, ellip1, tri1, butter with mods 1,5,10,20,50,100,200,400\n",
    "dists_avg = np.transpose([[0.004174072201,0.00539508588936,0.00406902624002,0.00561273291433],[0.00424745812569,0.0059672177021,0.0044816880945,0.00582550026999],[0.00371908713442,0.00511543824635,0.00371321917808,0.00491286851325],[0.00402253928778,0.00510835481356,0.00405491690961,0.00511058820482],[0.00364162855923,0.00483592603532,0.00363702164553,0.00523133984924],[0.00383869051779,0.00477036059801,0.004284059003,0.00489811435511],[0.00407818530496,0.00577834449012,0.00414091982503,0.00508186111089],[0.00376513128265,0.00569862031285,0.00400732900878,0.00513413780176]])*1000\n",
    "dists_std = np.transpose([[0.00259115737967,0.00416847071369,0.0025910897571,0.004019517648],[0.00248442641551,0.00355023655479,0.00280208671003,0.00607067084104],[0.00222420980624,0.00332107492258,0.00250263294026,0.00306031644755],[0.00321723098336,0.00377211435231,0.00248781832546,0.00373984746373],[0.00212480873796,0.00346466418531,0.00207069510347,0.00550469333458],[0.00230905423479,0.00300393992134,0.00242575958624,0.00327214707882],[0.00249478325989,0.00338054593903,0.00260663151293,0.00348589586966],[0.00215905836733,0.00363459768816,0.00224951820607,0.00336522636219]])*1000/np.sqrt(350)\n",
    "rot_avg = np.transpose([[0.252274843628,0.366062875924,0.264648902447,0.390669092195],[0.121372220378,0.195967766555,0.133392494195,0.167594886496],[0.107112309389,0.192605699812,0.123241465214,0.147309826954],[0.125262553663,0.208092778887,0.148133068346,0.182191689339],[0.114813439128,0.195663087689,0.117820021479,0.150275371795],[0.109302035738,0.180553580156,0.106860850896,0.139552317543],[0.123026070942,0.202021760767,0.124767310681,0.164774265556],[0.114779675876,0.1966752851,0.12398435151,0.162222983751]])\n",
    "rot_std = np.transpose([[0.90067701073,1.13597599466,0.969728610913,1.18679226056],[0.176088307103,0.368047900846,0.265838159459,0.268763659631],[0.156569416959,0.383231924843,0.232517310955,0.22299063132],[0.186312993554,0.357767015242,0.227646049125,0.241558193308],[0.202555995106,0.448384368264,0.249450561399,0.262237078417],[0.160691750962,0.33472618728,0.186005455388,0.191406685614],[0.239778402977,0.432832441615,0.227494917001,0.305563147895],[0.181019167684,0.358298909743,0.231339061904,0.228280870148]])/np.sqrt(350)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dist and rotation errors from shape gen results of rect1, ellip1, tri1, butter with mods 1,5,10,20,50,100,200,400\n",
    "dists_avg = np.transpose([[0.004174072201,0.00539508588936,0.00406902624002,0.00561273291433],[0.00424745812569,0.0059672177021,0.0044816880945,0.00582550026999],[0.00371908713442,0.00511543824635,0.00371321917808,0.00491286851325],[0.00402253928778,0.00510835481356,0.00405491690961,0.00511058820482],[0.00364162855923,0.00483592603532,0.00363702164553,0.00523133984924],[0.00383869051779,0.00477036059801,0.004284059003,0.00489811435511],[0.00407818530496,0.00577834449012,0.00414091982503,0.00508186111089],[0.00376513128265,0.00569862031285,0.00400732900878,0.00513413780176]])*1000\n",
    "dists_std = np.transpose([[0.00259115737967,0.00416847071369,0.0025910897571,0.004019517648],[0.00248442641551,0.00355023655479,0.00280208671003,0.00607067084104],[0.00222420980624,0.00332107492258,0.00250263294026,0.00306031644755],[0.00321723098336,0.00377211435231,0.00248781832546,0.00373984746373],[0.00212480873796,0.00346466418531,0.00207069510347,0.00550469333458],[0.00230905423479,0.00300393992134,0.00242575958624,0.00327214707882],[0.00249478325989,0.00338054593903,0.00260663151293,0.00348589586966],[0.00215905836733,0.00363459768816,0.00224951820607,0.00336522636219]])*1000/np.sqrt(350)\n",
    "rot_avg = np.transpose([[0.252274843628,0.366062875924,0.264648902447,0.390669092195],[0.121372220378,0.195967766555,0.133392494195,0.167594886496],[0.107112309389,0.192605699812,0.123241465214,0.147309826954],[0.125262553663,0.208092778887,0.148133068346,0.182191689339],[0.114813439128,0.195663087689,0.117820021479,0.150275371795],[0.109302035738,0.180553580156,0.106860850896,0.139552317543],[0.123026070942,0.202021760767,0.124767310681,0.164774265556],[0.114779675876,0.1966752851,0.12398435151,0.162222983751]])\n",
    "rot_std = np.transpose([[0.90067701073,1.13597599466,0.969728610913,1.18679226056],[0.176088307103,0.368047900846,0.265838159459,0.268763659631],[0.156569416959,0.383231924843,0.232517310955,0.22299063132],[0.186312993554,0.357767015242,0.227646049125,0.241558193308],[0.202555995106,0.448384368264,0.249450561399,0.262237078417],[0.160691750962,0.33472618728,0.186005455388,0.191406685614],[0.239778402977,0.432832441615,0.227494917001,0.305563147895],[0.181019167684,0.358298909743,0.231339061904,0.228280870148]])/np.sqrt(350)\n",
    "\n",
    "m = [1,5,10,20,50,100,200,400]\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(m, dists_avg[0], yerr=dists_std[0], marker='s', color='b')\n",
    "plt.errorbar(m, dists_avg[1], yerr=dists_std[1], marker='o', color='r')\n",
    "plt.errorbar(m, dists_avg[2], yerr=dists_std[2], marker='^', color='g')\n",
    "plt.errorbar(m, dists_avg[3], yerr=dists_std[3], marker='x', color='c')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(m, rot_avg[0], yerr=rot_std[0], marker='s', color='b')\n",
    "plt.errorbar(m, rot_avg[1], yerr=rot_std[1], marker='o', color='r')\n",
    "plt.errorbar(m, rot_avg[2], yerr=rot_std[2], marker='^', color='g')\n",
    "plt.errorbar(m, rot_avg[3], yerr=rot_std[3], marker='x', color='c')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dist and rotation errors from shape gen results of rect1, ellip1, tri1, butter with mods 1,5,10,20,50,100,200,400\n",
    "dists_avg = np.transpose([[0.004174072201,0.00539508588936,0.00406902624002,0.00561273291433],[0.00424745812569,0.0059672177021,0.0044816880945,0.00582550026999],[0.00371908713442,0.00511543824635,0.00371321917808,0.00491286851325],[0.00402253928778,0.00510835481356,0.00405491690961,0.00511058820482],[0.00364162855923,0.00483592603532,0.00363702164553,0.00523133984924],[0.00383869051779,0.00477036059801,0.004284059003,0.00489811435511]])*1000\n",
    "dists_std = np.transpose([[0.00259115737967,0.00416847071369,0.0025910897571,0.004019517648],[0.00248442641551,0.00355023655479,0.00280208671003,0.00607067084104],[0.00222420980624,0.00332107492258,0.00250263294026,0.00306031644755],[0.00321723098336,0.00377211435231,0.00248781832546,0.00373984746373],[0.00212480873796,0.00346466418531,0.00207069510347,0.00550469333458],[0.00230905423479,0.00300393992134,0.00242575958624,0.00327214707882]])*1000/np.sqrt(350)\n",
    "rot_avg = np.transpose([[0.252274843628,0.366062875924,0.264648902447,0.390669092195],[0.121372220378,0.195967766555,0.133392494195,0.167594886496],[0.107112309389,0.192605699812,0.123241465214,0.147309826954],[0.125262553663,0.208092778887,0.148133068346,0.182191689339],[0.114813439128,0.195663087689,0.117820021479,0.150275371795],[0.109302035738,0.180553580156,0.106860850896,0.139552317543]])\n",
    "rot_std = np.transpose([[0.90067701073,1.13597599466,0.969728610913,1.18679226056],[0.176088307103,0.368047900846,0.265838159459,0.268763659631],[0.156569416959,0.383231924843,0.232517310955,0.22299063132],[0.186312993554,0.357767015242,0.227646049125,0.241558193308],[0.202555995106,0.448384368264,0.249450561399,0.262237078417],[0.160691750962,0.33472618728,0.186005455388,0.191406685614]])/np.sqrt(350)\n",
    "\n",
    "m = [1,5,10,20,50,100]\n",
    "fig, ax = plt.subplots(figsize=(3, 3.5))\n",
    "rect1 = ax.errorbar(m, dists_avg[0], yerr=dists_std[0], marker='s', color='b', capsize=3)\n",
    "ellip1 = ax.errorbar(m, dists_avg[1], yerr=dists_std[1], marker='o', color='r', capsize=3)\n",
    "tri1 = ax.errorbar(m, dists_avg[2], yerr=dists_std[2], marker='^', color='g', capsize=3)\n",
    "butter = ax.errorbar(m, dists_avg[3], yerr=dists_std[3], marker='x', color='c', capsize=3)\n",
    "ax.set_ylabel('Avg Translation Error (mm)')\n",
    "ax.set_xlabel('Number of Kernels')\n",
    "#ax.legend((rect1[0], ellip1[0], tri1[0], butter[0]), ('rect1', 'ellip1', 'tri1', 'butter'))\n",
    "plt.show()\n",
    "\n",
    "fig, ax2 = plt.subplots(figsize=(3, 3.5))\n",
    "rect1 = ax2.errorbar(m, rot_avg[0], yerr=rot_std[0], marker='s', color='b', capsize=3)\n",
    "ellip1 = ax2.errorbar(m, rot_avg[1], yerr=rot_std[1], marker='o', color='r', capsize=3)\n",
    "tri1 = ax2.errorbar(m, rot_avg[2], yerr=rot_std[2], marker='^', color='g', capsize=3)\n",
    "butter = ax2.errorbar(m, rot_avg[3], yerr=rot_std[3], marker='x', color='c', capsize=3)\n",
    "rotationname = 'Avg Rotation Error (' + r'$\\theta$)'\n",
    "ax2.set_xlabel('Number of Kernels')\n",
    "ax2.set_ylabel(rotationname)\n",
    "ax2.legend((rect1[0], ellip1[0], tri1[0], butter[0]), ('rect1', 'ellip1', 'tri1', 'butter'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
